---
title: 'Homework 6: Time Series'
author: "Paul Harmon & Justin Gomez"
date: "October 18, 2016"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(grDevices)
```

__The following code will count the days in Bozeman where the minimum temperature was measured to be below 32 degrees F (0 degrees C) and the number of days where information was available in `Data1`.__
```{r,warning=F,message=F}
Bozeman<-read.csv("https://dl.dropboxusercontent.com/u/77307195/Bozeman.csv",header=T)

monthsF<-sort(unique(Bozeman$MonthRE))
countfun<-function(x) c(sum(x<32),sum(!is.na(x)))

  monthcountMINF<-aggregate(Bozeman$TMIN..F.,by=list(Bozeman$MonthRE),FUN=countfun)
  yearcountMINF<-aggregate(Bozeman$TMIN..F.,by=list(Bozeman$Year),FUN=countfun)
  
  Data1<-data.frame(Year=yearcountMINF[,1],DaysBelow32=yearcountMINF$x[,1],
        MeasuredDays=yearcountMINF$x[,2],PropDays=yearcountMINF$x[,1]/yearcountMINF$x[,2])
```  

1) __Make nice looking and labeled time series plots of the number of days below freezing and the proportion of measured days below freezing.__ 

```{r problem 1}
plot(Data1$Year,Data1$DaysBelow32,type="l",col="steelblue2",lwd=2,main="Number of Days Below Freezing per Year", xlab="Year",ylab="Number of Days")

plot(Data1$Year,Data1$PropDays,type="l",col="slateblue1",lwd=2,main="Proportion of Days Below Freezing per Year",xlab="Year",ylab="Proportion of Days")
```

2) __Estimate a linear trend model for the proportion of measured days below freezing and report the parametric (t-test) linear trend test results in a sentence. Also discuss scope of inference for this test in a sentence or two (random sampling and random assignment and their implications).__

The linear trend model for the proportion of measured days below freezing is given below. The t-test results are also given.  

Testing for a linear trend in the proportion of days below freezing with a t-test yields a test statistic of -3.173 on 107 degrees of freedom, with an associated p-value of 0.00197, so we have strong evidence that there is a linear trend in the proportion of days below freezing.

The data refer only to the Bozeman area, and to a specific period of years measured between 1900 and 2008. We did not randomly choose the site, nor did we randomly select the time period used to observe the climate there. In fact, since this is a purely observational study we can neither make causal inference nor inferences to the population. Thus, the scope of inference for the above t-test is only for the area of Bozeman during the 108 year period specified. To use this result on future (or past) years, or for a different site, would be an unjustified extrapolation. 

```{r problem2}
#estimates a linear time trend model
mod.1 <- lm(PropDays~ Year, data = Data1)
summary(mod.1)
```

3) __Discuss this proportion response versus using the count of days below zero per year, specific to this example and in general. What issues does using one or the other present?__ 

Looking at a proportion helps to eliminate any effect that the difference in the number of days observed may have. Some of the earlier years, especially early on in this time frame, don't have a full year's worth of observations, so just looking at a count of days below freezing may give us the wrong impression as to what is going on in these years.

4) _Generate a permutation test for the trend with the proportion response. I performed one in the syllabus (page 6) using the `shuffle` function from the `mosaic` package. Report a plot of the permutation distribution, the test statistic you used, and a p-value. Generally randomization based tests are more robust to violations of the normality assumption as long as the distribution (shape and variability) is the same for all observations except for differences in the center or mean. Why would that be advantageous with this response?_

The permutation distribution can be observed below in both the histrogram and the density curve. The red vertical line in both of these plots represents our observed result. This observed result is our test statistic, and it yields a p-value of 0.001, indicating that we have strong evidence for the linear time trend.

When we permute the observations, we get an idea of what the trend would look like if the order was random. Doing a permutation test, then, shows us that there is indeed some sort of time trend present in the data. 
```{r problem 4}
library(mosaic)
B<- 1000 
Tstar<-matrix(NA,nrow=B) 
  
for (b in (1:B)){ 
  Tstar[b]<-lm(PropDays~shuffle(Year),data=Data1)$coefficients[2] 
  } 
par(mfrow=c(1,2)) 
Tobs<-lm(PropDays~Year,data=Data1)$coefficients[2] 
Tobs
hist(Tstar,labels=FALSE, col = "skyblue") 
abline(v=Tobs,lwd=2,col="red") 
plot(density(Tstar),main="Density curve of Tstar") 
abline(v=Tobs,lwd=2,col="red")
pdata(abs(Tstar),abs(Tobs),lower.tail=F)
```
 


5) _The Sen estimator or, more commonly, Theil-Sen is based on a single median of all the possible pairwise generated slopes. Its standard version is available in the `mblm` (median based linear models) R package developed by Lukasz Komsta. The package description provides more details (https://cran.r-project.org/web/packages/mblm/mblm.pdf). Note that with `mblm`, you need to use `repeated=FALSE` to get the Theil-Sen estimator and not the better estimator developed by Siegel. The package has a `summary` function that provides a test based on the nonparametric Wilcox test but it had terrible Type I error rates when I explored it. Without further explorations, I would recommend avoiding its use. Fortunately, our permutation approach can be used to develop a test based on the Theil-Sen slope coefficient. First, compare the estimated slope provided by `mblm` to what you found from the linear model and its permutation test. Then develop a permutation test based on the slope coefficient from `mblm` - note that `mblm` conveniently has the same output structure as `lm`. The confidence interval that runs on `mblm` seems to perform well enough to study, so we can make 95% confidence intervals and check whether 0 is in the interval or not as the following code suggests to use it to perform our 5% significance level hypothesis test._

The slope coefficients for both functions are very similar to each other. It appears that using the least-squares method, we obtain a coefficient of -0.00031. Using the Theil-Sen estimator, we get a slope coefficient of -0.00032. When we compute the permutation test using the Theil-Sen coefficient, we obtain a p-value that is very small. This indicates strong evidence in favor of a linear time trend.  

```{r,warning=F,message=F, problem 5}
if(!require('mblm')){install.packages('mblm')}
library(mblm)
model1s<-mblm(PropDays~Year,data=Data1,repeated=FALSE)
summary(model1s)
confint(model1s)
CI<-confint(model1s)[2,] #Extract CI and check whether 0 is in interval
(0>CI[1])&(0<CI[2]) #If 0 is in interval, FTR H0

#compare the coefficients
Tobs
summary(model1s)$coefficients[2]
#Pretty darn close

#Permutation test
A<-1000
TstarA<-rep(12,A) 
there<-logical(A)
for (a in (1:A)){ 
  Data1$that<-shuffle(Data1$Year)
  model<-mblm(PropDays~that,data=Data1,repeated=FALSE)
  TstarA[a]<-model$coefficients[2]
  CI<-confint(model)[2,]
  there[a]<-(0>CI[1])&(0<CI[2])
}
FTR<-sum(there==TRUE)
R<-sum(there==FALSE)
R/A
```

6) _Use the residual error variance estimate from your linear model for the proportion responses to simulate a series with no trend (constant mean and you can leave it at 0) and normal white noise with that same variance. Use that simulation code to perform a simulation study of the Type I error rate for the parametric t-test for the slope coefficient, the test using the confidence interval from `mblm`, and your permutation test (use 500 permutations and do 250 simulations to keep the run time somewhat manageable). Report the simulation-based Type I error rates when using a 5% significance level test for the three procedures with the same sample size as the original data set._

  - _For the parametric test, the p-value can be extracted from the `lm`  model `summary`'s using `summary(model1)$coef[2,4]`._
  
  - _It is best and easiest if you do one loop for the simulations and then for each simulated data set in each loop generate the three test results, extracting the p-values that each produces. If you struggle to set this up, please send me an email or stop by with an attempt at your code for some feedback._
  
  - _This will be computationally intensive. To avoid needing to re-run results in R-markdown, you can try the `cache=T` option for any of the permutation or simulation code chunks. Or for this section, you can just report the three error rates and comment out the code you used._

The error rates for the parametric t test, confidence interval check with mblm, and permutation test are given in the output below as sim.lm, sim.mblm, and perm.result, respectively.   
  

```{r problem 6 cache=TRUE}
num.sims=250
num.perms=500
x=seq(1,109,by=1)
happened<-logical(length=num.sims)
w<-as.data.frame(cbind(seq(1,109,by=1),rep(12,109)))
colnames(w)<-c("index","noise")
coef<-rep(12,num.sims)
inthere<-logical(length=num.sims)
scoef<-rep(12,num.perms)
perm.p<-rep(12,num.sims)
for(i in 1:num.sims) {
  w[,2]<-rnorm(109,mean=0,sd=sd(lm(Data1$PropDays~Data1$Year)$residuals))
  model.lm<-lm(noise~index,data=w)
  coef[i]<-model.lm$coefficients[2]
  pvalue.nt<-summary(model.lm)$coefficients[2,4]
  happened[i]<-pvalue.nt<.05
  model.mblm<-mblm(noise~index,data=w,repeated=FALSE)
  CI<-confint(model.mblm)[2,]
  inthere[i]<-(0>CI[1])&(0<CI[2])
  for(j in 1:num.perms) {
    w$shuf<-shuffle(w$index)
    scoef[j]<-lm(noise~shuf,data=w)$coefficients[2]
  }
  obs<-lm(noise~index,data=w)$coefficients[2]
  perm.p[i]<-pdata(abs(scoef),abs(obs),lower.tail=FALSE)
}
sim.lm<-sum(happened==TRUE)/num.sims
sim.lm
sim.mblm<-sum(inthere==FALSE)/num.sims
sim.mblm
perm.result<-sum((perm.p<.05)==TRUE)/num.sims
perm.result
```
7) _Instead of white noise errors, we might also be interested in Type I error rates when we have autocorrelation present (again with no trend in the true process). Use the results for an AR(1) process variance (derived in class) to calculate the white noise variance needed to generate a process with the same variance as you used for your previous simulation, but when_ $\phi=0.3$ _and_ $0.6$. _In other words_, $gamma_{0}$ _of the AR(1) process needs to match the white noise variance used above and the white noise process driving the AR(1) process needs to be adjusted appropriately_.

  - _Show your derivation of the required white noise variances first for $\phi=0.3$ and $\phi=0.6$._ 
    
  - _To simulate the process we can use this value in the `arima.sim` function in something like `arima.sim(n=2000,list(ar=c(0.3)),sd=5)` where `n=2000` provides 2000 simulated observations, `model=list(ar=c(0.3))` determines that we are using an AR(1) process with parameter of of 0.3, and `sd=5` controls the SD of the normal white noise used to build the AR(1) process_ __(this is not the variance of the AR(1) process)__. _Check that you get about your expected results using something like:_

We use the formula given in class $\gamma_{0} = \frac{\sigma^{2}_{e}}{(1-\phi^{2})}$ to find the variance of the white noise process using an AR(1) structure. For $\phi = .3$, $\gamma_{0} =  0.001$, which closely matches the variance from the _arima.sim_ function.  For $\phi = 0.6$, the $\gamma_{0} = 0.002$, which again closely matches the variance from the _arima.sim_ function. 
  
  
```{r,warning=F,message=F}
phi<-.3
gamma<-summary(model.lm)$sigma^2/(1-phi^2)
ar1sim<-arima.sim(n=2000,model=list(ar=c(phi)),sd=summary(model.lm)$sigma)
var(ar1sim) 

phi<-.6
gamma<-summary(model.lm)$sigma^2/(1-phi^2)
ar1sim<-arima.sim(n=2000,model=list(ar=c(phi)),sd=summary(model.lm)$sigma)
var(ar1sim) 
```

8) _Repeat your simulation study of the parametric, permutation, and Theil-Sen linear trend test based on the CI. Report the estimated Type I error rates in the presence of AR(1) correlations with a parameter of 0.6 based on your work in the previous question for simulating the response time series. Discuss the impacts of having autocorrelation present on the various procedures._

When we repeat the simulation study, we see that the estimated Type I error rates appear to change when autcorrelation is present, as would be expected. The parametric test error rate increased, as did the permutation test error rate; however, the mblm error rate actually decreased in the presence of autocorrelation. 


```{r probelm 8 cache=TRUE}
num.sims=250
num.perms=500
x=seq(1,109,by=1)
happened<-logical(length=num.sims)
ar<-as.data.frame(cbind(seq(1,109,by=1),rep(12,109)))
colnames(ar)<-c("index","noise")
coef<-rep(12,num.sims)
inthere<-logical(length=num.sims)
scoef<-rep(12,num.perms)
perm.p<-rep(12,num.sims)
for(i in 1:num.sims) {
  ar[,2]<-arima.sim(n=109,model=list(ar=c(.6)),sd=summary(model.lm)$sigma)
  model.lm<-lm(noise~index,data=ar)
  coef[i]<-model.lm$coefficients[2]
  pvalue.nt<-summary(model.lm)$coefficients[2,4]
  happened[i]<-pvalue.nt<.05
  model.mblm<-mblm(noise~index,data=ar,repeated=FALSE)
  CI<-confint(model.mblm)[2,]
  inthere[i]<-(0>CI[1])&(0<CI[2])
  for(j in 1:num.perms) {
    ar$shuf<-shuffle(ar$index)
    scoef[j]<-lm(noise~shuf,data=ar)$coefficients[2]
  }
  obs<-lm(noise~index,data=ar)$coefficients[2]
  perm.p[i]<-pdata(abs(scoef),abs(obs),lower.tail=FALSE)
}
sim.lm<-sum(happened==TRUE)/num.sims
sim.lm
sim.mblm<-sum(inthere==FALSE)/num.sims
sim.mblm
perm.result<-sum((perm.p<.05)==TRUE)/num.sims
perm.result
```

9) _The Zhang method you read about is also available in the `zyp` package but it only provides confidence intervals and I am not completely convinced by their discussion of the intervals provided without more exploration. But you can get estimates from `zyp.sen` and confidence intervals using `confint.zyp` on the results from `zyp.sen`. The `confint` function can also be applied to `mblm` results. Find and compare the two confidence intervals for the Sen-estimators for the proportion response time series. No simulation study here - just complete the analysis._

The Zhang method is given below. We include the confidence intervals below. Both methods give intervals that _do not_ contain $0$; however, the interval coming from the Zyp package is slightly wider than the interval fro the MBLM method. In either case, it appears that the proportion of days below freezing is decreasing over time - both methods tell this same story. 

```{r}
library(zyp)
CI
mod.sen <- zyp.sen(PropDays~Year,data=Data1)
confint.zyp(mod.sen)[2,] #gives the confidence interval for the year componenent
```



10) _Make a plot of the original proportion response time series with the parametric linear, Theil-Sen, and Zhang methods/models on the same plot. You may want to use `plot(y~x,type="l")` and then add lines to the plot._

The plot of the original proportion response time series is compared with the linear, Theil-Sen, and Zhang methods on the same plot below. The Zhang line is given by the orange-shaded line, the Theil-Sen is given by the green line, and the linear model is given by the blue dotted line.  The models are calculated in different ways, but in this case the regression lines are essentially the same for all three methods.  

```{r}
par(mfrow = c(1,1))
plot(Data1$PropDays~Data1$Year, type = "l", col = "tomato2", lwd = 2, main = "Comparison of Estimates", xlab= "Year", ylab = "Proportion of Days")
abline(coef = mod.sen$coefficients, col = rgb(.9,.1,0,.2), lwd = 10)
abline(model1s, lty = 1, col = "limegreen", lwd = 2) #mblm
abline(mod.1, lty = 2, col = "blue2", lwd = 2) #linear model
legend("topright", fill = c(rgb(.9,.1,0.2), "limegreen", "blue2"), legend = c("Zhang","Theil-Sen","Linear"))
```





